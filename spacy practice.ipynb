{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp =spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Yo bay ! U.S. just fucked russia in the ass for $5 and five pounds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo 96 PROPN compound Yo\n",
      "bay 96 PROPN ROOT bay\n",
      "! 97 PUNCT punct !\n",
      "U.S. 96 PROPN nsubj U.S.\n",
      "just 86 ADV advmod just\n",
      "fucked 100 VERB ROOT fuck\n",
      "russia 96 PROPN dobj russia\n",
      "in 85 ADP prep in\n",
      "the 90 DET det the\n",
      "ass 92 NOUN pobj ass\n",
      "for 85 ADP prep for\n",
      "$ 99 SYM nmod $\n",
      "5 93 NUM nummod 5\n",
      "and 89 CCONJ cc and\n",
      "five 93 NUM conj five\n",
      "pounds 92 NOUN pobj pound\n"
     ]
    }
   ],
   "source": [
    "for item in doc:\n",
    "    print(item.text, item.pos, item.pos_, item.dep_, item.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x27314c23a88>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x27314c0dac8>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x27314c280a8>)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.tokenizer.Tokenizer at 0x27314b18ac8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(\"okay Ceasar, we're almost there! 2 miles from athens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ceasar \n",
      " PERSON \n",
      " People, including fictional \n",
      "\n",
      "\n",
      "2 miles \n",
      " QUANTITY \n",
      " Measurements, as of weight or distance \n",
      "\n",
      "\n",
      "athens \n",
      " GPE \n",
      " Countries, cities, states \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in doc2.ents:\n",
    "    print(token, \"\\n\", token.label_,\"\\n\", str(spacy.explain(token.label_)), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okay Ceasar, we're almost there!\n",
      "2 miles from athens\n",
      "we\n",
      "2 miles\n",
      "athens\n"
     ]
    }
   ],
   "source": [
    "for sent in doc2.sents:\n",
    "    print(sent)\n",
    "for noun in doc2.noun_chunks:\n",
    "    print(noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc2, style='dep', jupyter = True, options={'distance':75})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc2, style='ent', jupyter = True, options={'distance':75})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.serve(doc2, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pStemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run-->    run\n",
      "ran-->    ran\n",
      "runs-->    run\n",
      "running-->    run\n",
      "runner-->    runner\n",
      "cools-->    cool\n",
      "cooling-->    cool\n",
      "easily-->    easili\n",
      "fairly-->    fairli\n",
      "fairness-->    fair\n",
      "generous-->    gener\n",
      "generate-->    gener\n",
      "generation-->    gener\n",
      "generously-->    gener\n"
     ]
    }
   ],
   "source": [
    "words = [\"run\", \"ran\", \"runs\", \"running\", \"runner\", \"cools\", \"cooling\", \"easily\", \"fairly\", \"fairness\",\n",
    "        \"generous\", \"generate\", \"generation\", \"generously\"]\n",
    "for word in words:\n",
    "    print(word + \"-->    \" + pStemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowStemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run-->    run\n",
      "ran-->    ran\n",
      "runs-->    run\n",
      "running-->    run\n",
      "runner-->    runner\n",
      "cools-->    cool\n",
      "cooling-->    cool\n",
      "easily-->    easili\n",
      "fairly-->    fair\n",
      "fairness-->    fair\n",
      "generous-->    generous\n",
      "generate-->    generat\n",
      "generation-->    generat\n",
      "generously-->    generous\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \"-->    \" + snowStemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldoc = nlp(u'i am a runner running in a race because i love to run since i ran')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i                     PRON         5097672513440128799           i\n",
      "am                    AUX         10382539506755952630          be\n",
      "a                     DET         11901859001352538922           a\n",
      "runner                NOUN        12640964157389618806      runner\n",
      "running               VERB        12767647472892411841         run\n",
      "in                    ADP          3002984154512732771          in\n",
      "a                     DET         11901859001352538922           a\n",
      "race                  NOUN         8048469955494714898        race\n",
      "because               SCONJ       16950148841647037698     because\n",
      "i                     PRON         5097672513440128799           i\n",
      "love                  VERB         3702023516439754181        love\n",
      "to                    PART         3791531372978436496          to\n",
      "run                   VERB        12767647472892411841         run\n",
      "since                 SCONJ       10066841407251338481       since\n",
      "i                     PRON         5097672513440128799           i\n",
      "ran                   VERB        12767647472892411841         run\n"
     ]
    }
   ],
   "source": [
    "for token in ldoc:\n",
    "    print(f\"{token.text:{20}}  {token.pos_:{10}}  {token.lemma:{20}}  {token.lemma_:>{10}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doing', 'very', '’m', 'third', 'me', 'beforehand', 'throughout', 'twenty', 'before', 'once', 'i', 'becoming', 'she', 'against', 'between', 'hereafter', 'becomes', 'thru', 'might', 'put', 'serious', 'via', 'namely', '‘d', 'nevertheless', 'formerly', 'beyond', 'could', 'meanwhile', 'on', 'still', 'their', 'one', 'whereas', 'make', 'therefore', '‘ll', 'when', 'by', 'upon', 'unless', 'rather', 'whenever', 'none', 'six', 'three', 'each', 'enough', 'as', 'sometimes', 'both', 'if', 'did', 'that', 'always', 'seeming', 'whereby', 'show', 'thereupon', 'no', 'go', 'take', '’d', 'due', 'more', 'it', 'latterly', 'must', 'move', 'this', 'hers', 'sometime', 'are', 'nobody', 'anyway', 'ten', 'or', 'fifteen', 'just', 'together', \"'ve\", 'with', 'sixty', 'toward', 'why', \"'re\", 'do', 'may', 'part', 'anywhere', 'us', 'least', 'he', 'much', 'ours', 'back', 'front', 'her', 'latter', 'quite', 'were', 'into', 'afterwards', 'along', 'because', 'mine', 'few', 'can', 'will', 'except', 'over', \"n't\", 'to', 'was', 'its', 'which', 'well', 'what', 're', 'from', 'wherever', 'among', 'yours', 'last', 'so', 'himself', 'whence', 'we', 'done', 'thus', 'myself', 'mostly', 'otherwise', 'twelve', '’ve', 'often', 'without', 'although', 'hereupon', 'everyone', 'up', 'elsewhere', 'amongst', 'wherein', \"'d\", \"'m\", 'either', 'moreover', 'bottom', 'hence', 'should', '‘ve', 'call', 'below', 'for', 'ourselves', 'after', 'ever', 'name', 'see', 'almost', 'nothing', 'anyone', 'here', 'have', 'they', 'became', 'yourself', 'already', 'his', '‘s', 'used', 'seems', 'itself', 'also', '’ll', '‘m', 'others', 'ca', 'where', 'hundred', 'themselves', 'empty', 'there', 'around', 'how', 'above', 'across', 'eight', 'please', 'thereafter', 'while', 'during', 'other', 'made', 'somehow', 'at', 'down', 'out', 'per', 'than', 'yet', 'only', 'now', 'regarding', 'former', 'top', 'okay', 'anything', 'another', 'whither', 'cannot', 'does', 'nor', 'several', 'those', 'even', 'had', '‘re', 'all', \"'s\", 'seem', 'whatever', 'your', 'whom', 'fifty', 'you', 'is', 'forty', 'in', 'thereby', 'not', 'give', 'has', 'an', 'seemed', 'say', 'amount', 'whose', 'hereby', 'indeed', 'n‘t', 'through', 'whether', 'various', 'whereupon', 'everything', 'yourselves', 'get', 'something', 'really', 'until', 'whoever', 'and', 'again', 'besides', 'five', 'same', 'full', '’re', 'nine', 'within', 'alone', 'beside', 'become', \"'ll\", 'thence', 'anyhow', 'my', 'then', 'though', 'someone', 'who', 'everywhere', 'neither', 'any', 'keep', 'some', 'next', 'herself', 'being', 'eleven', 'therein', 'behind', 'many', 'noone', 'him', 'somewhere', 'onto', 'our', 'them', 'about', 'of', 'would', 'every', 'side', 'four', 'whole', 'off', 'two', 'but', 'never', 'first', 'nowhere', 'such', 'else', 'since', 'further', 'whereafter', 'however', 'own', 'under', '’s', 'less', 'most', 'using', 'n’t', 'am', 'been', 'the', 'be', 'too', 'perhaps', 'herein', 'towards', 'a', 'these'}\n"
     ]
    }
   ],
   "source": [
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(nlp.Defaults.stop_words))\n",
    "nlp.vocab['whatevs'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words.add('whatevs')\n",
    "nlp.vocab['whatevs'].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329\n"
     ]
    }
   ],
   "source": [
    "nlp.vocab['whatevs'].is_stop\n",
    "print(len(nlp.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
